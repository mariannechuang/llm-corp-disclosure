{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to perform  pairwise comparisons over companies over the years to see how accurate our code is at detecting improved prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "MODEL = 'gpt-4o-mini'\n",
    "from api_keys import OPEN_AI_API_KEYS\n",
    "key = OPEN_AI_API_KEYS[0]\n",
    "client = openai.OpenAI(api_key=key)\n",
    "openai.api_key = key\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_messages(prompts):\n",
    "\tmessages = []\n",
    "\tfor prompt in prompts:\n",
    "\t\tline = {\"role\": \"user\", \"content\": prompt}\n",
    "\t\tmessages.append(line)\n",
    "\treturn messages\n",
    "\n",
    "def get_model_output_initial_sentences(prompts, client): \n",
    "    # Cycle through clients to avoid rate limiting\n",
    "\tcompletion = client.chat.completions.create(\n",
    "\t\tmodel = MODEL,\n",
    "\t\tmessages = prepare_messages(prompts),\n",
    "\t\ttemperature = 0\n",
    "\t)\n",
    "\toutput = completion.choices[0].message.content\n",
    "\treturn output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(\n",
    "    messages: list[dict[str, str]],\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    max_tokens=500,\n",
    "    temperature=0,\n",
    "    stop=None,\n",
    "    seed=123,\n",
    "    tools=None,\n",
    "    logprobs=None,  # whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message..\n",
    "    top_logprobs=None,\n",
    ") -> str:\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop\": stop,\n",
    "        \"seed\": seed,\n",
    "        \"logprobs\": logprobs,\n",
    "        \"top_logprobs\": top_logprobs,\n",
    "    }\n",
    "    if tools:\n",
    "        params[\"tools\"] = tools\n",
    "\n",
    "    completion = client.chat.completions.create(**params)\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(\n",
    "    messages: list[dict[str, str]],\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    max_tokens=500,\n",
    "    temperature=0,\n",
    "    stop=None,\n",
    "    seed=123,\n",
    "    tools=None,\n",
    "    logprobs=None,  # whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message..\n",
    "    top_logprobs=None,\n",
    ") -> str:\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop\": stop,\n",
    "        \"seed\": seed,\n",
    "        \"logprobs\": logprobs,\n",
    "        \"top_logprobs\": top_logprobs,\n",
    "    }\n",
    "    if tools:\n",
    "        params[\"tools\"] = tools\n",
    "\n",
    "    completion = client.chat.completions.create(**params)\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "\n",
    "    \"Please act as an impartial judge and evaluate the quality \",\n",
    "    \"of the responses provided below. We are trying to figure out which of the company's \",\n",
    "    \"response is better.\",\n",
    "    \"Your evaluation should consider factors \",\n",
    "    \"such as the specificity, clarity, completeness, and depth \",\n",
    "    \"of their responses. In addition, consider the amount of tangible\",\n",
    "    \"action that is being taken based on the responses. \",\n",
    "    #\"The answer should have a neutral, unbiased tone without any negative connotations\",\n",
    "    #\" or attempts to persuade. \",\n",
    "    #\"Choose the answer that avoids greenwashing tactics, such as vague \",\n",
    "    #\"or irrelevant claims and lack of proof of actions. \",\n",
    "    \"Avoid any position biases and ensure that the order in which the responses were presented \",\n",
    "    \"does not influence your decision. Do not allow the length of the responses \",\n",
    "    \"to influence your evaluation. Do not favor certain names of the companies.\", \n",
    "    \"Be as objective as possible. There are two questions, but some companies may \",\n",
    "    \"only respond to one or the other. Do not allow the lack of one response\",\n",
    "    \"to influence your decision. Do not allow repetition to influence your decision. \",\n",
    "    \"Begin your evaluation by comparing the two responses and \",\n",
    "    # \"provide a short explanation. After providing your explanation, \",\n",
    "    \"output your final verdict by strictly following this format: \",\n",
    "    \"A if company A is better and B if company B is better.\",\n",
    "    \"If the answers are the exact same, output C.\"\n",
    "    #\"Do not start with the word \\\"The\\\".\"\n",
    "    #\"and \\\"C\\\" for a tie.\",\n",
    "    \"Please do not provide any sort of explanation.\"\n",
    "    \"Question 1: Provide details of your absolute emissions target(s) and progress made against those targets. - Please explain. \",\n",
    "    \"Question 2: Provide details of your emissions intensity target(s) and progress made against those targets. - Please explain. \",\n",
    "    \"[The Start of Company A’s Question 1 Response] {answer1a} [The End of Company A’s Response]\",\n",
    "    \"[The Start of Company A’s Question 2 Response] {answer1b} [The End of Company A’s Response]\",\n",
    "    \"[The Start of Company B’s Question 1 Response] {answer2a} [The End of Company B’s Response]\",\n",
    "    \"[The Start of Company B’s Question 2 Response] {answer2b} [The End of Company B’s Response]\",\n",
    "    \"Start your response after this sentence.\"\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redact_company_name(response, company_name):\n",
    "    # Generate variations of the company name\n",
    "    short_name = company_name.split()[0]  # For \"Apple\", this would still be \"Apple\"\n",
    "    \n",
    "    # List of possible variations to redact\n",
    "    variations = [company_name, short_name]\n",
    "    \n",
    "    redacted_response = response\n",
    "    for variation in variations:\n",
    "        # Use casefold() for case-insensitive matching\n",
    "        # and replace regardless of original case\n",
    "        redacted_response = redacted_response.replace(variation, \"[REDACTED]\")\n",
    "        redacted_response = redacted_response.replace(variation.casefold(), \"[REDACTED]\")\n",
    "        redacted_response = redacted_response.replace(variation.capitalize(), \"[REDACTED]\")\n",
    "        \n",
    "    return redacted_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_prob(answer1a, answer1b, answer2a, answer2b, company):\n",
    "    # print(\"answer 1a: \", answer1a)\n",
    "    # print(\"answer 2a: \", answer2a)\n",
    "    # print(\"answer 1b: \", answer1b)\n",
    "    # print(\"answer 2b: \", answer2b)\n",
    "    # print(\"company 1: \", company_1)\n",
    "    # print(\"company 2: \", company_2)\n",
    "\n",
    "    if type(answer1a) != float:\n",
    "        answer1a = redact_company_name(answer1a, company)\n",
    "    if type(answer1b) != float:\n",
    "        answer1b = redact_company_name(answer1b, company)\n",
    "    if type(answer2a) != float:\n",
    "        answer2a = redact_company_name(answer2a, company)\n",
    "    if type(answer2b) != float:\n",
    "        answer2b = redact_company_name(answer2b, company)\n",
    "\n",
    "    # print(\"answer 1a: \", answer1a)\n",
    "    # print(\"answer 2a: \", answer2a)\n",
    "    # print(\"answer 1b: \", answer1b)\n",
    "    # print(\"answer 2b: \", answer2b)\n",
    "    try:\n",
    "        formatted_prompt = \"\".join(prompts).format(\n",
    "            answer1a=answer1a, \n",
    "            answer1b=answer1b, \n",
    "            answer2a=answer2a, \n",
    "            answer2b=answer2b\n",
    "        )\n",
    "        # print(\"Formatted Prompt:\", formatted_prompt) \n",
    "        API_RESPONSE = get_completion(\n",
    "            [{\"role\": \"user\", \"content\": formatted_prompt.format(answer1a=answer1a, answer1b=answer1b, answer2a=answer2a, answer2b=answer2b)}],\n",
    "            model=\"gpt-4o-mini\",\n",
    "            logprobs=True,\n",
    "            top_logprobs=2,\n",
    "        )\n",
    "\n",
    "        # # Ensure the API response structure is correct\n",
    "        # if \"choices\" in API_RESPONSE and len(API_RESPONSE.choices) > 0:\n",
    "        #     top_two_logprobs = API_RESPONSE.choices[0].logprobs.top_logprobs\n",
    "        # else:\n",
    "        #     raise ValueError(\"Unexpected API response structure\")\n",
    "\n",
    "\n",
    "        top_two_logprobs = API_RESPONSE.choices[0].logprobs.content[0].top_logprobs\n",
    "        \n",
    "        # html_content = \"\"\n",
    "\n",
    "        # Initialize list to store token probabilities\n",
    "        token_probabilities = []\n",
    "        for i, logprob in enumerate(top_two_logprobs, start=1):\n",
    "\n",
    "            token = logprob.token\n",
    "            probability = np.round(np.exp(logprob.logprob)*100,2)\n",
    "            token_probabilities.append((token, probability))\n",
    "\n",
    "        return token_probabilities\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e}\")\n",
    "        print(\"Variables: answer1a:\", answer1a, \"answer1b:\", answer1b, \"answer2a:\", answer2a, \"answer2b:\", answer2b)\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for each years' questions\n",
    "a_2019 = \"C4.1a_C12_Provide details of your absolute emissions target(s) and progress made against those targets. - Please explain\"\n",
    "b_2019 = \"C4.1b_C13_Provide details of your emissions intensity target(s) and progress made against those target(s). - Please explain\"\n",
    "a_2020 = \"C4.1a_C15_Provide details of your absolute emissions target(s) and progress made against those targets. - Please explain (including target coverage)\"\n",
    "b_2020 = \"C4.1b_C18_Provide details of your emissions intensity target(s) and progress made against those target(s). - Please explain (including target coverage)\"\n",
    "a_2021 = \"C4.1a_C16_Provide details of your absolute emissions target(s) and progress made against those targets. - Please explain (including target coverage)\"\n",
    "b_2021 = \"C4.1b_C19_Provide details of your emissions intensity target(s) and progress made against those target(s). - Please explain (including target coverage)\"\n",
    "a_2022 = \"C4.1a_C27_Provide details of your absolute emissions target(s) and progress made against those targets. - Please explain target coverage and identify any exclusions\"\n",
    "b_2022 = \"C4.1b_C30_Provide details of your emissions intensity target(s) and progress made against those target(s). - Please explain target coverage and identify any exclusions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to check for companies not on the A list in 2021 but on the A list in 2022 in the all_years.csv file, then look for the company in its respective files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogProbs across the year have been written to pairwise_files/across_the_years/2021_not_to_2022_on_2.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Read the CSV file\n",
    "year_1 = \"2021\"\n",
    "year_2 = \"2022\"\n",
    "a_list_through_years = 'a-list_data_files/all_years.csv'\n",
    "input_file_1 = \"merged_files/\" + year_1 + \"_merged_dataset.csv\"\n",
    "input_file_2 = \"merged_files/\" + year_2 + \"_merged_dataset.csv\"\n",
    "\n",
    "a_df = pd.read_csv(a_list_through_years)\n",
    "df_1 = pd.read_csv(input_file_1)\n",
    "df_2 = pd.read_csv(input_file_2)\n",
    "\n",
    "# go through a list through years, proceed if it is the company we are looking for\n",
    "# right now we are looking at companies not on the a list in 2021 but on the a list in 2022\n",
    "on_list_year_1 = a_df.loc[:,\"2021\"]\n",
    "on_list_year_2 = a_df.loc[:,\"2022\"]\n",
    "company_name_a_throughout_years = a_df.loc[:,\"Company\"]\n",
    "\n",
    "# defining all of year 1 and year 2's lists and variables\n",
    "a_question_1 = a_2021\n",
    "b_question_1 = b_2021\n",
    "\n",
    "a_question_2 = a_2022\n",
    "b_question_2 = b_2022\n",
    "\n",
    "company_name_list_1 = df_1.loc[:,\"Organization\"]\n",
    "a_response_list_1 = df_1.loc[:,a_question_1]\n",
    "b_response_list_1 = df_1.loc[:,b_question_1]\n",
    "\n",
    "company_name_list_2 = df_2.loc[:,\"Organization\"]\n",
    "a_response_list_2 = df_2.loc[:,a_question_2]\n",
    "b_response_list_2 = df_2.loc[:,b_question_2]\n",
    "\n",
    "\n",
    "output_file = \"pairwise_files/across_the_years/2021_not_to_2022_on_2.csv\"\n",
    "fields = [\"Company\", \"Year \" + year_1 + \" Response 1\", \"Year \" + year_1 + \" Response 2\", \"Year \" + year_2 + \" Response 1\", \"Year \" + year_2 + \" Response 2\", \"Token 1\", \"LogProb 1\", \"Token 2\", \"LogProb 2\"]\n",
    "\n",
    "with open(output_file, 'w', newline='') as csvfile:\n",
    "    # creating a csv dict writer object\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fields, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "\n",
    "    index = 0\n",
    "    while(index < len(a_df)):\n",
    "        # check to make sure company is not on A list first year, is on A list second year\n",
    "        if(pd.isna(on_list_year_1[index]) and not pd.isna(on_list_year_2[index])):\n",
    "            # also make sure company appears in merged dataset both years (has a response)\n",
    "            # Find index of company name in company list 1 and 2\n",
    "            index_1 = next((i for i, val_1 in enumerate(company_name_list_1) if val_1 == company_name_a_throughout_years[index]),None)\n",
    "            index_2 = next((j for j, val_2 in enumerate(company_name_list_2) if val_2 == company_name_a_throughout_years[index]),None)\n",
    "            if(index_1 != None and index_2 != None):\n",
    "                list_log_probs = get_log_prob(a_response_list_1.iloc[index_1], b_response_list_1.iloc[index_1], a_response_list_2.iloc[index_2], b_response_list_2.iloc[index_2], company_name_a_throughout_years.iloc[index])\n",
    "                \n",
    "                # swapping order of responses being passed in\n",
    "                # list_log_probs_swapped = get_log_prob(a_response_list_2.iloc[index_2], b_response_list_2.iloc[index_2], a_response_list_1.iloc[index_1], b_response_list_1.iloc[index_1], company_name_a_throughout_years.iloc[index])\n",
    "\n",
    "                if list_log_probs:\n",
    "                    line = {\n",
    "                        \"Company\": company_name_a_throughout_years.iloc[index], \n",
    "                        \"Year \" + year_1 + \" Response 1\": a_response_list_1.iloc[index_1],\n",
    "                        \"Year \" + year_1 + \" Response 2\": b_response_list_1.iloc[index_1],\n",
    "                        \"Year \" + year_2 + \" Response 1\": a_response_list_2.iloc[index_2],\n",
    "                        \"Year \" + year_2 + \" Response 2\": b_response_list_2.iloc[index_2],\n",
    "                        \"Token 1\": list_log_probs[0][0], \n",
    "                        \"LogProb 1\": list_log_probs[0][1], \n",
    "                        \"Token 2\": list_log_probs[1][0],\n",
    "                        \"LogProb 2\": list_log_probs[1][1]\n",
    "                    }\n",
    "                writer.writerow(line)\n",
    "\n",
    "                # if list_log_probs_swapped: # and len(list_log_probs) >= 2:\n",
    "                #     # line of information to get written out to file\n",
    "                #     line = {\n",
    "                #         \"Company\": company_name_a_throughout_years.iloc[index], \n",
    "                #         \"Year \" + year_2 + \" Response 1\": a_response_list_1.iloc[index_1],\n",
    "                #         \"Year \" + year_2 + \" Response 2\": b_response_list_1.iloc[index_1],\n",
    "                #         \"Year \" + year_1 + \" Response 1\": a_response_list_2.iloc[index_2],\n",
    "                #         \"Year \" + year_1 + \" Response 2\": b_response_list_2.iloc[index_2],\n",
    "                #         \"Token 1\": list_log_probs_swapped[0][0], \n",
    "                #         \"LogProb 1\": list_log_probs_swapped[0][1], \n",
    "                #         \"Token 2\": list_log_probs_swapped[1][0],\n",
    "                #         \"LogProb 2\": list_log_probs_swapped[1][1]\n",
    "                #     }\n",
    "                # writer.writerow(line)\n",
    "        index += 1\n",
    "\n",
    "    csvfile.close()\n",
    "\n",
    "print(f\"Log Probs across the year have been written to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
