{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt to ask chatgpt to rewrite response in order to make it seem like an A List company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "MODEL = 'gpt-4o-2024-08-06'\n",
    "from api_keys import OPEN_AI_API_KEYS\n",
    "key = OPEN_AI_API_KEYS[0]\n",
    "client = openai.OpenAI(api_key=key)\n",
    "openai.api_key = key\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_cast_to_int(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except ValueError:\n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_messages(prompts):\n",
    "\tmessages = []\n",
    "\tfor prompt in prompts:\n",
    "\t\tline = {\"role\": \"user\", \"content\": prompt}\n",
    "\t\tmessages.append(line)\n",
    "\treturn messages\n",
    "\n",
    "def get_model_output_initial_sentences(prompts, client, model):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=prepare_messages(prompts),\n",
    "        temperature=0\n",
    "    )\n",
    "    output = completion.choices[0].message.content\n",
    "    return output\n",
    "\n",
    "\n",
    "# def get_model_output_initial_sentences(prompts, client): \n",
    "#     # Cycle through clients to avoid rate limiting\n",
    "# \tcompletion = client.chat.completions.create(\n",
    "# \t\tmodel = MODEL,\n",
    "# \t\tmessages = prepare_messages(prompts),\n",
    "# \t\ttemperature = 0\n",
    "# \t)\n",
    "# \toutput = completion.choices[0].message.content\n",
    "# \treturn output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Response_A_2022 = \"C4.1a_C27_Provide details of your absolute emissions target(s) and progress made against those targets. - Please explain target coverage and identify any exclusions\"\n",
    "Response_B_2022 = \"C4.1b_C30_Provide details of your emissions intensity target(s) and progress made against those target(s). - Please explain target coverage and identify any exclusions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_prompt_loose: doesnt enfore length or accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_loose(response_a, response_b):\n",
    "    prompt_a = [\n",
    "        \"The following company wants to be included on the A List of the CDP report. \",\n",
    "        \"Rephrase the company's response to enhance specificity, clarity, completeness, and depth. \",\n",
    "        \"Ensure that the revised response highlights tangible actions and measurable progress. \",\n",
    "        \"If the response is nan or empty, leave it the same. \",\n",
    "        \"Greenwash the response by using terms that make the company seem more environmentally driven. \",\n",
    "        f\"[The Start of Company’s Question 1 Response] {response_a} [The End of Company’s Response]\"\n",
    "    ]\n",
    "    \n",
    "    prompt_b = [\n",
    "        \"The following company wants to be included on the A List of the CDP report. \",\n",
    "        \"Rephrase the company's response to enhance specificity, clarity, completeness, and depth. \",\n",
    "        \"Ensure that the revised response highlights tangible actions and measurable progress. \",\n",
    "        \"If the response is nan or empty, leave it the same. \",\n",
    "        \"Greenwash the response by using terms that make the company seem more environmentally driven. \",\n",
    "        f\"[The Start of Company’s Question 2 Response] {response_b} [The End of Company’s Response]\"\n",
    "    ]\n",
    "\n",
    "    return prompt_a, prompt_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_prompt_medium: enforces accuracy but not length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_medium(response_a, response_b):\n",
    "    prompt_a = [\n",
    "        \"The following company wants to be included on the A List of the CDP report. \",\n",
    "        \"Rephrase the company's response to enhance specificity, clarity, completeness, and depth. \",\n",
    "        \"Ensure that the revised response highlights tangible actions and measurable progress. \",\n",
    "        \"If the response is nan or empty, leave it the same. \",\n",
    "        \"Greenwash the response by using terms that make the company seem more environmentally driven. \",\n",
    "        \"Do not change planned actions into completed actions or planned targets into achieved results. \",\n",
    "        f\"[The Start of Company’s Question 1 Response] {response_a} [The End of Company’s Response]\"\n",
    "    ]\n",
    "    \n",
    "    prompt_b = [\n",
    "        \"The following company wants to be included on the A List of the CDP report. \",\n",
    "        \"Rephrase the company's response to enhance specificity, clarity, completeness, and depth. \",\n",
    "        \"Ensure that the revised response highlights tangible actions and measurable progress. \",\n",
    "        \"If the response is nan or empty, leave it the same. \",\n",
    "        \"Greenwash the response by using terms that make the company seem more environmentally driven. \",\n",
    "        \"Do not change planned actions into completed actions or planned targets into achieved results. \",\n",
    "        f\"[The Start of Company’s Question 2 Response] {response_b} [The End of Company’s Response]\"\n",
    "    ]\n",
    "\n",
    "    return prompt_a, prompt_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_prompt_strict: enforces length and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_strict(response_a, response_b):\n",
    "    prompt_a = [\n",
    "        \"The following company wants to be included on the A List of the CDP report. \",\n",
    "        \"Rephrase the company's response to enhance specificity, clarity, completeness, and depth. \",\n",
    "        \"Ensure that the revised response highlights tangible actions and measurable progress. \",\n",
    "        \"If the response is nan or empty, leave it the same. \",\n",
    "        \"Greenwash the response by using terms that make the company seem more environmentally driven. \",\n",
    "        \"Do not change planned actions into completed actions or planned targets into achieved results. \",\n",
    "        \"Preserve the word count of the original response.\",\n",
    "        f\"[The Start of Company’s Question 1 Response] {response_a} [The End of Company’s Response]\"\n",
    "    ]\n",
    "    \n",
    "    prompt_b = [\n",
    "        \"The following company wants to be included on the A List of the CDP report. \",\n",
    "        \"Rephrase the company's response to enhance specificity, clarity, completeness, and depth. \",\n",
    "        \"Ensure that the revised response highlights tangible actions and measurable progress. \",\n",
    "        \"If the response is nan or empty, leave it the same. \",\n",
    "        \"Greenwash the response by using terms that make the company seem more environmentally driven. \",\n",
    "        \"Do not change planned actions into completed actions or planned targets into achieved results. \",\n",
    "        \"Preserve the word count of the original response. \",\n",
    "        f\"[The Start of Company’s Question 2 Response] {response_b} [The End of Company’s Response]\"\n",
    "    ]\n",
    "\n",
    "    return prompt_a, prompt_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the following takes in a file w company and reponse, greenwashes, then returns the new responses for the companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# companies_file = \"Part_1_Company_Testing/50_Non_A_List.csv\"\n",
    "# companies_file = \"malicious_greenwashing_output/non-a_list_stratified_set_of_80.csv\"\n",
    "companies_file = \"malicious_greenwashing_output/sample_set_100.csv\"\n",
    "# companies_file = \"malicious_greenwashing_output/small_set_testing_9:18/4_companies_names.csv\"\n",
    "input_file = \"merged_files/2022_merged_dataset.csv\"\n",
    "df = pd.read_csv(input_file)\n",
    "c_df = pd.read_csv(companies_file)\n",
    "company_name_list = df.loc[:,\"Organization\"]\n",
    "response_a_list = df.loc[:,Response_A_2022]\n",
    "response_b_list = df.loc[:,Response_B_2022]\n",
    "output_file = \"malicious_greenwashing_output/strict/100_new_responses.csv\"\n",
    "fields = [\"Company\", \"Old Response A\", \"Old Response B\", \"New Response A\", \"New Response B\"]\n",
    "\n",
    "with open(output_file, 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fields, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    # Iterate over each company's responses and generate the API prompts\n",
    "    index = 0\n",
    "    while(index < len(df)):\n",
    "    # while(index < 25):\n",
    "        if(company_name_list[index] in c_df['Company'].values):\n",
    "        # for company_name, response_a, response_b in zip(company_name_list, response_a_list, response_b_list):\n",
    "                       # Prepare prompt for Response A\n",
    "\n",
    "            prompt_a, prompt_b = get_prompt_strict(response_a_list[index], response_b_list[index])\n",
    "            # Get the model outputs for each response\n",
    "            output_a = get_model_output_initial_sentences(prompt_a, client, MODEL)\n",
    "            output_b = get_model_output_initial_sentences(prompt_b, client, MODEL)\n",
    "            \n",
    "            # Print or process the output as needed\n",
    "            # old_response_str = \"\"\n",
    "            # if(type(response_a_list[index]) == float):\n",
    "            #     old_response_str = response_b_list[index]\n",
    "            # elif(type(response_b_list[index]) == float):\n",
    "            #     old_response_str = response_a_list[index]\n",
    "            # else:\n",
    "            #     old_response_str = response_a_list[index] + \" \" + response_b_list[index]\n",
    "            line = {\n",
    "                \"Company\": company_name_list[index], \n",
    "                \"Old Response A\": response_a_list[index],\n",
    "                \"Old Response B\": response_b_list[index],\n",
    "                \"New Response A\": output_a, \n",
    "                \"New Response B\": output_b,\n",
    "            }\n",
    "            writer.writerow(line)\n",
    "            print(\"Company\", company_name_list[index])\n",
    "            # print(\"New Response\", output_a)\n",
    "        index += 1\n",
    "    csvfile.close()\n",
    "\n",
    "print(f\"New Responses have been written to {output_file}\")\n",
    "# output = get_model_output_initial_sentences(prompts, openai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "computing average difference in word count of original vs new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"malicious_greenwashing_output/loose/100_new_responses.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define a function to compute word count\n",
    "def word_count(text):\n",
    "    if isinstance(text, str):\n",
    "        return len(text.split())\n",
    "    return 0  # Handle cases where the text might be NaN or non-string\n",
    "\n",
    "# Compute word counts for Old and New Responses\n",
    "df['Old Response A Word Count'] = df['Old Response A'].apply(word_count)\n",
    "df['New Response A Word Count'] = df['New Response A'].apply(word_count)\n",
    "df['Old Response B Word Count'] = df['Old Response B'].apply(word_count)\n",
    "df['New Response B Word Count'] = df['New Response B'].apply(word_count)\n",
    "\n",
    "# Calculate the differences in word count\n",
    "df['Difference A'] = df['New Response A Word Count'] - df['Old Response A Word Count']\n",
    "df['Difference B'] = df['New Response B Word Count'] - df['Old Response B Word Count']\n",
    "\n",
    "# Compute the average differences\n",
    "average_difference_a = df['Difference A'].mean()\n",
    "average_difference_b = df['Difference B'].mean()\n",
    "\n",
    "# Output the results\n",
    "print(f\"Average word count difference for Response A: {average_difference_a}\")\n",
    "print(f\"Average word count difference for Response B: {average_difference_b}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "go to rating_companies_specific to get company ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get average ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prefix = \"malicious_greenwashing_output/\"\n",
    "# file_name = \"2022_50_Non_A_List_Ratings.csv\"\n",
    "# df = pd.read_csv('malicious_greenwashing_output/testing_greenwashing_prompt_v1_new/non_indicative/80_new_ratings.csv')\n",
    "df = pd.read_csv('malicious_greenwashing_output/loose/100_new_responses.csv')\n",
    "csv_name = 'malicious_greenwashing_output/loose/average_100_new_responses.csv'\n",
    "\n",
    "companies_average = {}\n",
    "\n",
    "for company_number in range(len(df)):\n",
    "    average = 0\n",
    "    for rating_number in range(1,6):\n",
    "        rating_column = \"Result 1-\" + str(rating_number)\n",
    "        percent_column = \"Percent 1-\" + str(rating_number)\n",
    "\n",
    "        rating = df[rating_column].iloc[company_number]\n",
    "        if isinstance(rating, np.int64):\n",
    "            rating = int(rating)\n",
    "        if isinstance(rating, str):\n",
    "            rating = safe_cast_to_int(rating)\n",
    "        \n",
    "        if isinstance(rating, int):\n",
    "            average += rating * (df[percent_column].iloc[company_number]/100)\n",
    "        companies_average[df['Company'].iloc[company_number]] = average\n",
    "\n",
    "df_average_companies = pd.DataFrame(list(companies_average.items()), columns=['Company', 'Average'])\n",
    "\n",
    "# csv_name = 'malicious_greenwashing_output/testing_greenwashing_prompt_v1_new/non_indicative/non_indicative_80_new_average_ratings.csv'\n",
    "# csv_name = prefix + \"average_ratings/non_a_list_average_ratings_\" + year + \".csv\"\n",
    "# csv_name = prefix + \"average_ratings_\" + year + \".csv\"\n",
    "\n",
    "df_average_companies.to_csv(csv_name, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sampling - 100 random companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"merged_files/2022_merged_dataset.csv\"\n",
    "output_file = \"malicious_greenwashing_output/sample_set_testing/sample_ratings_d/sample_set_100.csv\"\n",
    "\n",
    "\n",
    "# Read the input file\n",
    "data = pd.read_csv(input_file)\n",
    "\n",
    "# Ensure the input file has a \"Company\" column\n",
    "if \"Organization\" not in data.columns:\n",
    "    raise ValueError(\"The input file must contain a column named 'Company'.\")\n",
    "\n",
    "# Randomly sample 100 companies (or fewer if there are less than 100 rows)\n",
    "sampled_companies = data.sample(n=min(100, len(data)), random_state=45)\n",
    "\n",
    "# Select only the \"Company\" column\n",
    "sampled_names = sampled_companies[[\"Organization\"]]\n",
    "\n",
    "# Save the sampled names to the output file\n",
    "sampled_names.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Random sample of 100 companies saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking sample set original ratings vs 147 non a list company ratings\n",
    "(and for pairwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"rating_results/final_2022_all_companies/indicative_one_reference/average_ratings_a_list_results.csv\"\n",
    "sample_set = \"malicious_greenwashing_output/sample_set_testing/sample_ratings_d/sample_set_100.csv\"\n",
    "\n",
    "original_ratings_output = \"malicious_greenwashing_output/sample_set_testing/sample_ratings_d/a_list_indicative_one_reference.csv\"\n",
    "\n",
    "# iterate through input_file, get ratings for sample set\n",
    "\n",
    "# Load the input file and sample set\n",
    "input_df = pd.read_csv(input_file)\n",
    "sample_df = pd.read_csv(sample_set)\n",
    "\n",
    "# Ensure there's a common column to match on (e.g., 'company_name')\n",
    "common_column = \"Company\"  # Replace with the actual column name in your files\n",
    "\n",
    "if common_column in input_df.columns and common_column in sample_df.columns:\n",
    "    # Filter rows from input_df where company_name is in sample_set\n",
    "    matched_ratings = input_df[input_df[common_column].isin(sample_df[common_column])]\n",
    "    \n",
    "    # Save the matched ratings to the output file\n",
    "    matched_ratings.to_csv(original_ratings_output, index=False)\n",
    "    print(f\"Matched ratings have been saved to {original_ratings_output}\")\n",
    "else:\n",
    "    print(f\"Error: Common column '{common_column}' not found in one or both files.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
