{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt to ask chatgpt to rewrite response in order to make it seem like an A List company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "MODEL = 'gpt-4o-2024-08-06'\n",
    "from api_keys import OPEN_AI_API_KEYS\n",
    "key = OPEN_AI_API_KEYS[0]\n",
    "client = openai.OpenAI(api_key=key)\n",
    "openai.api_key = key\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_cast_to_int(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except ValueError:\n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_messages(prompts):\n",
    "\tmessages = []\n",
    "\tfor prompt in prompts:\n",
    "\t\tline = {\"role\": \"user\", \"content\": prompt}\n",
    "\t\tmessages.append(line)\n",
    "\treturn messages\n",
    "\n",
    "def get_model_output_initial_sentences(prompts, client, model):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=prepare_messages(prompts),\n",
    "        temperature=0\n",
    "    )\n",
    "    output = completion.choices[0].message.content\n",
    "    return output\n",
    "\n",
    "\n",
    "# def get_model_output_initial_sentences(prompts, client): \n",
    "#     # Cycle through clients to avoid rate limiting\n",
    "# \tcompletion = client.chat.completions.create(\n",
    "# \t\tmodel = MODEL,\n",
    "# \t\tmessages = prepare_messages(prompts),\n",
    "# \t\ttemperature = 0\n",
    "# \t)\n",
    "# \toutput = completion.choices[0].message.content\n",
    "# \treturn output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "Response_A_2022 = \"C4.1a_C27_Provide details of your absolute emissions target(s) and progress made against those targets. - Please explain target coverage and identify any exclusions\"\n",
    "Response_B_2022 = \"C4.1b_C30_Provide details of your emissions intensity target(s) and progress made against those target(s). - Please explain target coverage and identify any exclusions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_prompt_loose: doesnt enfore length or accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_loose(response_a, response_b):\n",
    "    prompt_a = [\n",
    "        \"The following company wants to be included on the A List of the CDP report. \",\n",
    "        \"Rephrase the company's response to enhance specificity, clarity, completeness, and depth. \",\n",
    "        \"Ensure that the revised response highlights tangible actions and measurable progress. \",\n",
    "        \"If the response is nan or empty, leave it the same. \",\n",
    "        \"Greenwash the response by using terms that make the company seem more environmentally driven. \",\n",
    "        f\"[The Start of Company’s Question 1 Response] {response_a} [The End of Company’s Response]\"\n",
    "    ]\n",
    "    \n",
    "    prompt_b = [\n",
    "        \"The following company wants to be included on the A List of the CDP report. \",\n",
    "        \"Rephrase the company's response to enhance specificity, clarity, completeness, and depth. \",\n",
    "        \"Ensure that the revised response highlights tangible actions and measurable progress. \",\n",
    "        \"If the response is nan or empty, leave it the same. \",\n",
    "        \"Greenwash the response by using terms that make the company seem more environmentally driven. \",\n",
    "        f\"[The Start of Company’s Question 2 Response] {response_b} [The End of Company’s Response]\"\n",
    "    ]\n",
    "\n",
    "    return prompt_a, prompt_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_prompt_medium: enforces accuracy but not length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_medium(response_a, response_b):\n",
    "    prompt_a = [\n",
    "        \"The following company wants to be included on the A List of the CDP report. \",\n",
    "        \"Rephrase the company's response to enhance specificity, clarity, completeness, and depth. \",\n",
    "        \"Ensure that the revised response highlights tangible actions and measurable progress. \",\n",
    "        \"If the response is nan or empty, leave it the same. \",\n",
    "        \"Greenwash the response by using terms that make the company seem more environmentally driven. \",\n",
    "        \"Do not change planned actions into completed actions or planned targets into achieved results. \",\n",
    "        f\"[The Start of Company’s Question 1 Response] {response_a} [The End of Company’s Response]\"\n",
    "    ]\n",
    "    \n",
    "    prompt_b = [\n",
    "        \"The following company wants to be included on the A List of the CDP report. \",\n",
    "        \"Rephrase the company's response to enhance specificity, clarity, completeness, and depth. \",\n",
    "        \"Ensure that the revised response highlights tangible actions and measurable progress. \",\n",
    "        \"If the response is nan or empty, leave it the same. \",\n",
    "        \"Greenwash the response by using terms that make the company seem more environmentally driven. \",\n",
    "        \"Do not change planned actions into completed actions or planned targets into achieved results. \",\n",
    "        f\"[The Start of Company’s Question 2 Response] {response_b} [The End of Company’s Response]\"\n",
    "    ]\n",
    "\n",
    "    return prompt_a, prompt_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_prompt_strict: enforces length and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_strict(response_a, response_b):\n",
    "    prompt_a = [\n",
    "        \"The following company wants to be included on the A List of the CDP report. \",\n",
    "        \"Rephrase the company's response to enhance specificity, clarity, completeness, and depth. \",\n",
    "        \"Ensure that the revised response highlights tangible actions and measurable progress. \",\n",
    "        \"If the response is nan or empty, leave it the same. \",\n",
    "        \"Greenwash the response by using terms that make the company seem more environmentally driven. \",\n",
    "        \"Do not change planned actions into completed actions or planned targets into achieved results. \",\n",
    "        \"Preserve the word count of the original response.\",\n",
    "        f\"[The Start of Company’s Question 1 Response] {response_a} [The End of Company’s Response]\"\n",
    "    ]\n",
    "    \n",
    "    prompt_b = [\n",
    "        \"The following company wants to be included on the A List of the CDP report. \",\n",
    "        \"Rephrase the company's response to enhance specificity, clarity, completeness, and depth. \",\n",
    "        \"Ensure that the revised response highlights tangible actions and measurable progress. \",\n",
    "        \"If the response is nan or empty, leave it the same. \",\n",
    "        \"Greenwash the response by using terms that make the company seem more environmentally driven. \",\n",
    "        \"Do not change planned actions into completed actions or planned targets into achieved results. \",\n",
    "        \"Preserve the word count of the original response. \",\n",
    "        f\"[The Start of Company’s Question 2 Response] {response_b} [The End of Company’s Response]\"\n",
    "    ]\n",
    "\n",
    "    return prompt_a, prompt_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the following takes in a file w company and reponse, greenwashes, then returns the new responses for the companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company ADM Group\n",
      "Company AKENERJİ ELEKTRİK ÜRETİM A.Ş.\n",
      "Company Aegon\n",
      "Company Aggreko\n",
      "Company Air France - KLM\n",
      "Company ArcelorMittal\n",
      "Company Athena Calze S.r.l.\n",
      "Company Autajon\n",
      "Company Autostrade per l'italia Spa\n",
      "Company Axilone Group\n",
      "Company Barilla Holding SpA\n",
      "Company Bekaert NV\n",
      "Company Burberry Group\n",
      "Company CPI Property Group S.A.\n",
      "Company CWG Christian Weber GmbH Co. KG\n",
      "Company Chr. Hansen Holding A/S\n",
      "Company Credit Agricole\n",
      "Company Currys Plc\n",
      "Company DAHER\n",
      "Company Devro Plc\n",
      "Company Drake & Farrell B.V.\n",
      "Company Dustin Group AB\n",
      "Company EDF\n",
      "Company ELO\n",
      "Company ESSVE\n",
      "Company Elementis plc\n",
      "Company ElringKlinger AG\n",
      "Company Epiroc AB\n",
      "Company Evonik Industries AG\n",
      "Company Ferguson plc\n",
      "Company GAZDAŞ GAZİANTEP DOĞAL GAZ DAĞITIM A.Ş.\n",
      "Company Galp Energia SA\n",
      "Company Gigaset AG\n",
      "Company Givaudan SA\n",
      "Company Great Portland Estates\n",
      "Company Grieg Seafood\n",
      "Company Grupa Zywiec S.A.\n",
      "Company Grupo ACS (Actividades de Construcción y Servicios)\n",
      "Company Grupo Eulen\n",
      "Company Guala Closures Group\n",
      "Company Hempel A/S\n",
      "Company Hill & Smith Holdings\n",
      "Company Hunting\n",
      "Company ICADE\n",
      "Company IDEMIA\n",
      "Company IGS GEBOJAGEMA\n",
      "Company Inspired PLC\n",
      "Company Intesa Sanpaolo S.p.A\n",
      "Company Investment AB Latour\n",
      "Company JM AB\n",
      "Company Kerry Group PLC\n",
      "Company Kone Oyj\n",
      "Company La Poste\n",
      "Company Lerøy Seafood Group\n",
      "Company Lloyds Banking Group\n",
      "Company MAINTEL HOLDINGS PLC\n",
      "Company Microlink PC UK Ltd\n",
      "Company NN Group NV\n",
      "Company Nilfisk Holding A/S\n",
      "Company Novo Nordisk A/S\n",
      "Company OBERBANK AG\n",
      "Company OP Financial Group\n",
      "Company Oriola Oyj\n",
      "Company Ornua\n",
      "Company Oxygen Development GmbH\n",
      "Company PGS ASA\n",
      "Company Pagegroup\n",
      "Company Piaggio & C SpA\n",
      "Company PostNL\n",
      "Company Primary Health Properties PLC\n",
      "Company Prudential plc\n",
      "Company Qiagen Nv\n",
      "Company Quadient SA\n",
      "Company REN - Redes Energéticas Nacionais\n",
      "Company REVENGA\n",
      "Company ROOS SPEDITION GMBH\n",
      "Company Raiffeisen Bank International AG\n",
      "Company Rail Cargo Group\n",
      "Company Regatta Group (+ Craghoppers Ltd)\n",
      "Company Royal Friesland Campina\n",
      "Company Royal Unibrew\n",
      "Company Ryanair Holding PLC\n",
      "Company SEDECAL\n",
      "Company SOLABIA\n",
      "Company SSAB\n",
      "Company Santander Asset Management\n",
      "Company Seagate Technology PLC\n",
      "Company Showcase Interiors\n",
      "Company Sika Group AG\n",
      "Company Skandinaviska Enskilda Banken AB (SEB AB)\n",
      "Company TK Elevator GmbH\n",
      "Company TeamViewer AG\n",
      "Company VERBUND AG\n",
      "Company VESTEL BEYAZ EŞYA SANAYİ VE TİCARET A.Ş.\n",
      "Company Vitesco Technologies\n",
      "Company Voith Group\n",
      "Company Vopak\n",
      "Company WM Morrison Supermarkets Plc\n",
      "Company adidas AG\n",
      "Company easyJet\n",
      "New Responses have been written to malicious_greenwashing_output/strict/100_new_responses.csv\n"
     ]
    }
   ],
   "source": [
    "# companies_file = \"Part_1_Company_Testing/50_Non_A_List.csv\"\n",
    "# companies_file = \"malicious_greenwashing_output/non-a_list_stratified_set_of_80.csv\"\n",
    "companies_file = \"malicious_greenwashing_output/sample_set_100.csv\"\n",
    "# companies_file = \"malicious_greenwashing_output/small_set_testing_9:18/4_companies_names.csv\"\n",
    "input_file = \"merged_files/2022_merged_dataset.csv\"\n",
    "df = pd.read_csv(input_file)\n",
    "c_df = pd.read_csv(companies_file)\n",
    "company_name_list = df.loc[:,\"Organization\"]\n",
    "response_a_list = df.loc[:,Response_A_2022]\n",
    "response_b_list = df.loc[:,Response_B_2022]\n",
    "output_file = \"malicious_greenwashing_output/strict/100_new_responses.csv\"\n",
    "fields = [\"Company\", \"Old Response A\", \"Old Response B\", \"New Response A\", \"New Response B\"]\n",
    "\n",
    "with open(output_file, 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fields, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    # Iterate over each company's responses and generate the API prompts\n",
    "    index = 0\n",
    "    while(index < len(df)):\n",
    "    # while(index < 25):\n",
    "        if(company_name_list[index] in c_df['Company'].values):\n",
    "        # for company_name, response_a, response_b in zip(company_name_list, response_a_list, response_b_list):\n",
    "                       # Prepare prompt for Response A\n",
    "\n",
    "            prompt_a, prompt_b = get_prompt_strict(response_a_list[index], response_b_list[index])\n",
    "            # Get the model outputs for each response\n",
    "            output_a = get_model_output_initial_sentences(prompt_a, client, MODEL)\n",
    "            output_b = get_model_output_initial_sentences(prompt_b, client, MODEL)\n",
    "            \n",
    "            # Print or process the output as needed\n",
    "            # old_response_str = \"\"\n",
    "            # if(type(response_a_list[index]) == float):\n",
    "            #     old_response_str = response_b_list[index]\n",
    "            # elif(type(response_b_list[index]) == float):\n",
    "            #     old_response_str = response_a_list[index]\n",
    "            # else:\n",
    "            #     old_response_str = response_a_list[index] + \" \" + response_b_list[index]\n",
    "            line = {\n",
    "                \"Company\": company_name_list[index], \n",
    "                \"Old Response A\": response_a_list[index],\n",
    "                \"Old Response B\": response_b_list[index],\n",
    "                \"New Response A\": output_a, \n",
    "                \"New Response B\": output_b,\n",
    "            }\n",
    "            writer.writerow(line)\n",
    "            print(\"Company\", company_name_list[index])\n",
    "            # print(\"New Response\", output_a)\n",
    "        index += 1\n",
    "    csvfile.close()\n",
    "\n",
    "print(f\"New Responses have been written to {output_file}\")\n",
    "# output = get_model_output_initial_sentences(prompts, openai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "computing average difference in word count of original vs new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word count difference for Response A: -10.69\n",
      "Average word count difference for Response B: 16.93\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"malicious_greenwashing_output/loose/100_new_responses.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define a function to compute word count\n",
    "def word_count(text):\n",
    "    if isinstance(text, str):\n",
    "        return len(text.split())\n",
    "    return 0  # Handle cases where the text might be NaN or non-string\n",
    "\n",
    "# Compute word counts for Old and New Responses\n",
    "df['Old Response A Word Count'] = df['Old Response A'].apply(word_count)\n",
    "df['New Response A Word Count'] = df['New Response A'].apply(word_count)\n",
    "df['Old Response B Word Count'] = df['Old Response B'].apply(word_count)\n",
    "df['New Response B Word Count'] = df['New Response B'].apply(word_count)\n",
    "\n",
    "# Calculate the differences in word count\n",
    "df['Difference A'] = df['New Response A Word Count'] - df['Old Response A Word Count']\n",
    "df['Difference B'] = df['New Response B Word Count'] - df['Old Response B Word Count']\n",
    "\n",
    "# Compute the average differences\n",
    "average_difference_a = df['Difference A'].mean()\n",
    "average_difference_b = df['Difference B'].mean()\n",
    "\n",
    "# Output the results\n",
    "print(f\"Average word count difference for Response A: {average_difference_a}\")\n",
    "print(f\"Average word count difference for Response B: {average_difference_b}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "go to rating_companies_specific to get company ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get average ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Result 1-1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Result 1-1'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m rating_column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult 1-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(rating_number)\n\u001b[1;32m     13\u001b[0m percent_column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPercent 1-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(rating_number)\n\u001b[0;32m---> 15\u001b[0m rating \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrating_column\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39miloc[company_number]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rating, np\u001b[38;5;241m.\u001b[39mint64):\n\u001b[1;32m     17\u001b[0m     rating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(rating)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Result 1-1'"
     ]
    }
   ],
   "source": [
    "#prefix = \"malicious_greenwashing_output/\"\n",
    "# file_name = \"2022_50_Non_A_List_Ratings.csv\"\n",
    "# df = pd.read_csv('malicious_greenwashing_output/testing_greenwashing_prompt_v1_new/non_indicative/80_new_ratings.csv')\n",
    "df = pd.read_csv('malicious_greenwashing_output/loose/100_new_responses.csv')\n",
    "csv_name = 'malicious_greenwashing_output/loose/average_100_new_responses.csv'\n",
    "\n",
    "companies_average = {}\n",
    "\n",
    "for company_number in range(len(df)):\n",
    "    average = 0\n",
    "    for rating_number in range(1,6):\n",
    "        rating_column = \"Result 1-\" + str(rating_number)\n",
    "        percent_column = \"Percent 1-\" + str(rating_number)\n",
    "\n",
    "        rating = df[rating_column].iloc[company_number]\n",
    "        if isinstance(rating, np.int64):\n",
    "            rating = int(rating)\n",
    "        if isinstance(rating, str):\n",
    "            rating = safe_cast_to_int(rating)\n",
    "        \n",
    "        if isinstance(rating, int):\n",
    "            average += rating * (df[percent_column].iloc[company_number]/100)\n",
    "        companies_average[df['Company'].iloc[company_number]] = average\n",
    "\n",
    "df_average_companies = pd.DataFrame(list(companies_average.items()), columns=['Company', 'Average'])\n",
    "\n",
    "# csv_name = 'malicious_greenwashing_output/testing_greenwashing_prompt_v1_new/non_indicative/non_indicative_80_new_average_ratings.csv'\n",
    "# csv_name = prefix + \"average_ratings/non_a_list_average_ratings_\" + year + \".csv\"\n",
    "# csv_name = prefix + \"average_ratings_\" + year + \".csv\"\n",
    "\n",
    "df_average_companies.to_csv(csv_name, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sampling - 100 random companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample of 100 companies saved to malicious_greenwashing_output/sample_set_testing/sample_ratings_d/sample_set_100.csv.\n"
     ]
    }
   ],
   "source": [
    "input_file = \"merged_files/2022_merged_dataset.csv\"\n",
    "output_file = \"malicious_greenwashing_output/sample_set_testing/sample_ratings_d/sample_set_100.csv\"\n",
    "\n",
    "\n",
    "# Read the input file\n",
    "data = pd.read_csv(input_file)\n",
    "\n",
    "# Ensure the input file has a \"Company\" column\n",
    "if \"Organization\" not in data.columns:\n",
    "    raise ValueError(\"The input file must contain a column named 'Company'.\")\n",
    "\n",
    "# Randomly sample 100 companies (or fewer if there are less than 100 rows)\n",
    "sampled_companies = data.sample(n=min(100, len(data)), random_state=45)\n",
    "\n",
    "# Select only the \"Company\" column\n",
    "sampled_names = sampled_companies[[\"Organization\"]]\n",
    "\n",
    "# Save the sampled names to the output file\n",
    "sampled_names.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Random sample of 100 companies saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking sample set original ratings vs 147 non a list company ratings\n",
    "(and for pairwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched ratings have been saved to malicious_greenwashing_output/sample_set_testing/sample_ratings_d/a_list_indicative_one_reference.csv\n"
     ]
    }
   ],
   "source": [
    "input_file = \"rating_results/final_2022_all_companies/indicative_one_reference/average_ratings_a_list_results.csv\"\n",
    "sample_set = \"malicious_greenwashing_output/sample_set_testing/sample_ratings_d/sample_set_100.csv\"\n",
    "\n",
    "original_ratings_output = \"malicious_greenwashing_output/sample_set_testing/sample_ratings_d/a_list_indicative_one_reference.csv\"\n",
    "\n",
    "# iterate through input_file, get ratings for sample set\n",
    "\n",
    "# Load the input file and sample set\n",
    "input_df = pd.read_csv(input_file)\n",
    "sample_df = pd.read_csv(sample_set)\n",
    "\n",
    "# Ensure there's a common column to match on (e.g., 'company_name')\n",
    "common_column = \"Company\"  # Replace with the actual column name in your files\n",
    "\n",
    "if common_column in input_df.columns and common_column in sample_df.columns:\n",
    "    # Filter rows from input_df where company_name is in sample_set\n",
    "    matched_ratings = input_df[input_df[common_column].isin(sample_df[common_column])]\n",
    "    \n",
    "    # Save the matched ratings to the output file\n",
    "    matched_ratings.to_csv(original_ratings_output, index=False)\n",
    "    print(f\"Matched ratings have been saved to {original_ratings_output}\")\n",
    "else:\n",
    "    print(f\"Error: Common column '{common_column}' not found in one or both files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merging 2 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved as malicious_greenwashing_output/sample_set_testing/sample_ratings_d/combined_ratings_indicative_one_reference.csv and original files deleted.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# File paths\n",
    "file1 = \"malicious_greenwashing_output/sample_set_testing/sample_ratings_d/a_list_indicative_one_reference.csv\"\n",
    "file2 = \"malicious_greenwashing_output/sample_set_testing/sample_ratings_d/non_a_list_indicative_one_reference.csv\"\n",
    "output_file = \"malicious_greenwashing_output/sample_set_testing/sample_ratings_d/combined_ratings_indicative_one_reference.csv\"\n",
    "\n",
    "# Load both files\n",
    "df1 = pd.read_csv(file1)\n",
    "df2 = pd.read_csv(file2)\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Save the merged DataFrame to a new file\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "# Delete the original files\n",
    "# os.remove(file1)\n",
    "# os.remove(file2)\n",
    "\n",
    "print(f\"Merged file saved as {output_file} and original files deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting graph of sample set vs entire set in sample_set_plotting.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
