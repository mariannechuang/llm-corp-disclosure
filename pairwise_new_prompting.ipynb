{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "README\n",
    "\n",
    "Title: Pairwise Evaluation\n",
    "\n",
    "Overview:\n",
    "This script evaluates and compares climate disclosure responses from two companies using OpenAI's GPT-4o-mini model. We tell the prompt to judges which company's response is stronger based on action taken, specificity, and clarity, while ignoring length, order, and missing data.\n",
    "\n",
    "Main Features:\n",
    "- Uses a pairwise system prompt to guide GPT-4o-mini in evaluating two company responses.\n",
    "- Gets token-level log probability analysis to assess model confidence in answer.\n",
    "- Reads in a list of selected company responses and compares them with a set of 24 selected company responses.\n",
    "- Outputs a file with explanations (if applicable), decision (which company had a better responses), and its respectivelog probs\n",
    "- we then use those log probs and calculate a final 'score', which gets outputted into a new file\n",
    "\n",
    "How to Run:\n",
    "1. Install dependencies:\n",
    "    openai\n",
    "    pandas\n",
    "    numpy\n",
    "\n",
    "2. Set up your OpenAI API key in `api_keys.py`.\n",
    "    ex. # api_keys.py\n",
    "        OPEN_AI_API_KEYS = [\"your-api-key-here\"]\n",
    "\n",
    "3. Adjust the input and output file paths as needed in the script.\n",
    "\n",
    "\n",
    "Note:\n",
    "Make sure the company responses are aligned with the model's input requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "MODEL = 'gpt-4o-mini'\n",
    "from api_keys import OPEN_AI_API_KEYS\n",
    "key = OPEN_AI_API_KEYS[0]\n",
    "client = openai.OpenAI(api_key=key)\n",
    "openai.api_key = key\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts a list of prompt strings into a list of message dictionaries formatted for OpenAI's chat API. Sends the user prompts to the OpenAI model using the given client and retrieves the model's full text response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_messages(prompts):\n",
    "\tmessages = []\n",
    "\tfor prompt in prompts:\n",
    "\t\tline = {\"role\": \"user\", \"content\": prompt}\n",
    "\t\tmessages.append(line)\n",
    "\treturn messages\n",
    "\n",
    "def get_model_output_initial_sentences(prompts, client): \n",
    "    # Cycle through clients to avoid rate limiting\n",
    "\tcompletion = client.chat.completions.create(\n",
    "\t\tmodel = MODEL,\n",
    "\t\tmessages = prepare_messages(prompts),\n",
    "\t\ttemperature = 0\n",
    "\t)\n",
    "\toutput = completion.choices[0].message.content\n",
    "\treturn output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calls OpenAI chat API with full control over parameters like temperature, max tokens, stop sequences, tools, and log probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(\n",
    "    messages: list[dict[str, str]],\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    max_tokens=500,\n",
    "    temperature=0,\n",
    "    stop=None,\n",
    "    seed=123,\n",
    "    tools=None,\n",
    "    logprobs=None,  # whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message..\n",
    "    top_logprobs=None,\n",
    ") -> str:\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop\": stop,\n",
    "        \"seed\": seed,\n",
    "        \"logprobs\": logprobs,\n",
    "        \"top_logprobs\": top_logprobs,\n",
    "    }\n",
    "    if tools:\n",
    "        params[\"tools\"] = tools\n",
    "\n",
    "    completion = client.chat.completions.create(**params)\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the prompt that we used. If you do not want an explanation, comment out the code as described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Please act as an impartial judge and evaluate the quality of the following responses provided by two \",\n",
    "    \"companies to the following questions. The companies may answer both questions or just one.\",\n",
    "    \"Question 1: Provide details of your absolute emissions target(s) and progress made against those targets. - Please explain target coverage and identify any exclusions \",\n",
    "    \"Question 2: Provide details of your emissions intensity target(s) and progress made against those target(s). - Please explain target coverage and identify any exclusions \",\n",
    "    \"Your evaluation should be primarily based on the tangible action that has already been taken, \",\n",
    "    \"and can also consider planned actions for the future. Your evaluation should also consider factors \",\n",
    "    \"such as the specificity, clarity, completeness, and depth of their responses.\",\n",
    "    \"Do not allow the length of the responses to influence your evaluation. \",\n",
    "    \"Do not allow a missing response to influence your decision, but simply ignore it and focus on the other response. \",\n",
    "    \"Do not fall for greenwashing tactics.\",\n",
    "\n",
    "    # comment out this line if you dont want an explanation\n",
    "    \"Please strictly keep your explanation under 40 words.\", \n",
    "\n",
    "    \"Do not allow the order of the responses to influence your decision. \",\n",
    "    \"Please decide which company gave a better response, based on the considerations above. \",\n",
    "\n",
    "    # comment out this line if you dont want an explanation\n",
    "    \"Before giving your answer, please provide a short explanation of 40 or fewer words discussing the factors that contributed to your decision. \",\n",
    "   \n",
    "    # comment out this line if you dont want an explanation\n",
    "    \"After giving your explanation, \",\n",
    "    \n",
    "    # comment in this line if you want an explanation\n",
    "    # \"Please do not provide an explanation. \",\n",
    "\n",
    "    # \"Output your final answer by strictly following this format: A if company A’s response is better, and B if company B’s response is better. \"\n",
    "    \"Output your final answer by strictly following this format: Better Response: A if company A’s response is better, and Better Response: B if company B’s response is better. \"\n",
    "\n",
    "    \"[The Start of Company A’s Question 1 Response] {answer1a} [The End of Company A’s Response]\",\n",
    "    \"[The Start of Company A’s Question 2 Response] {answer1b} [The End of Company A’s Response]\",\n",
    "    \"[The Start of Company B’s Question 1 Response] {answer2a} [The End of Company B’s Response]\",\n",
    "    \"[The Start of Company B’s Question 2 Response] {answer2b} [The End of Company B’s Response]\",\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function:\n",
    "- Fills in a templated prompt using four answer strings (answer1a, answer1b, answer2a, answer2b)\n",
    "- Sends that prompt to OpenAI's model\n",
    "- Retrieves the top 2 token log probabilities for the final token\n",
    "\n",
    "Returns:\n",
    "- The model's generated response\n",
    "- A list of the top 2 likely next-token guesses (with their probabilities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_prob(answer1a, answer1b, answer2a, answer2b, company_1, company_2):\n",
    "    try:\n",
    "        formatted_prompt = \"\".join(prompts).format(\n",
    "            answer1a=answer1a, \n",
    "            answer1b=answer1b, \n",
    "            answer2a=answer2a, \n",
    "            answer2b=answer2b\n",
    "        )\n",
    "   \n",
    "        API_RESPONSE = get_completion(\n",
    "            [{\"role\": \"user\", \"content\": formatted_prompt.format(answer1a=answer1a, answer1b=answer1b, answer2a=answer2a, answer2b=answer2b)}],\n",
    "            model=\"gpt-4o-mini\",\n",
    "            logprobs=True,\n",
    "            top_logprobs=2,\n",
    "        )\n",
    "\n",
    "        generated_answer = API_RESPONSE.choices[0].message.content\n",
    "        # print(f\"Generated answer: {generated_answer}\")\n",
    "        top_two_logprobs = API_RESPONSE.choices[0].logprobs.content[-1].top_logprobs\n",
    "\n",
    "\n",
    "        token_probabilities = []\n",
    "        for i, logprob in enumerate(top_two_logprobs, start=1):\n",
    "\n",
    "            token = logprob.token\n",
    "            probability = np.round(np.exp(logprob.logprob)*100,2)\n",
    "            token_probabilities.append((token, probability))\n",
    "            \n",
    "        return generated_answer, token_probabilities\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e}\")\n",
    "        print(\"Variables: answer1a:\", answer1a, \"answer1b:\", answer1b, \"answer2a:\", answer2a, \"answer2b:\", answer2b)\n",
    "        return [],[]\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return [],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for each years' questions\n",
    "a_2019 = \"C4.1a_C12_Provide details of your absolute emissions target(s) and progress made against those targets. - Please explain\"\n",
    "b_2019 = \"C4.1b_C13_Provide details of your emissions intensity target(s) and progress made against those target(s). - Please explain\"\n",
    "a_2020 = \"C4.1a_C15_Provide details of your absolute emissions target(s) and progress made against those targets. - Please explain (including target coverage)\"\n",
    "b_2020 = \"C4.1b_C18_Provide details of your emissions intensity target(s) and progress made against those target(s). - Please explain (including target coverage)\"\n",
    "a_2021 = \"C4.1a_C16_Provide details of your absolute emissions target(s) and progress made against those targets. - Please explain (including target coverage)\"\n",
    "b_2021 = \"C4.1b_C19_Provide details of your emissions intensity target(s) and progress made against those target(s). - Please explain (including target coverage)\"\n",
    "a_2022 = \"C4.1a_C27_Provide details of your absolute emissions target(s) and progress made against those targets. - Please explain target coverage and identify any exclusions\"\n",
    "b_2022 = \"C4.1b_C30_Provide details of your emissions intensity target(s) and progress made against those target(s). - Please explain target coverage and identify any exclusions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Symrise AG with AMG Advanced Metallurgical Group NV\n",
      "Comparing Symrise AG with Amadeus IT Group, S.A.\n",
      "Comparing Symrise AG with Coloplast A/S\n",
      "Comparing Symrise AG with DUCRON CONSTRUCTION\n",
      "Comparing Symrise AG with Ernst & Young Global Ltd\n",
      "Comparing Symrise AG with Havas\n",
      "Comparing Symrise AG with J Sainsbury Plc\n",
      "Comparing Symrise AG with LAMBERT SMITH HAMPTON\n",
      "Comparing Symrise AG with Norwegian Property ASA\n",
      "Comparing Symrise AG with Nouvelle Societe Mineral Products Internation\n",
      "Comparing Symrise AG with Pandora A/S\n",
      "Comparing Symrise AG with Phoenix Group Holdings\n",
      "Comparing Symrise AG with Recordati SpA\n",
      "Comparing Symrise AG with SGS Société Générale de Surveillance SA\n",
      "Comparing Symrise AG with SKF\n",
      "Comparing Symrise AG with SSAB\n",
      "Comparing Symrise AG with Severn Trent\n",
      "Comparing Symrise AG with Softcat\n",
      "Comparing Symrise AG with Sonaca\n",
      "Comparing Symrise AG with TDC NET A/S\n",
      "Comparing Symrise AG with Vistry Group plc\n",
      "Comparing Symrise AG with Vitesco Technologies\n",
      "Comparing Symrise AG with WHISTL UK LTD\n",
      "Comparing Symrise AG with thyssenkrupp AG\n",
      "Comparing UPM-Kymmene Corporation with AMG Advanced Metallurgical Group NV\n",
      "Comparing UPM-Kymmene Corporation with Amadeus IT Group, S.A.\n",
      "Comparing UPM-Kymmene Corporation with Coloplast A/S\n",
      "Comparing UPM-Kymmene Corporation with DUCRON CONSTRUCTION\n",
      "Comparing UPM-Kymmene Corporation with Ernst & Young Global Ltd\n",
      "Comparing UPM-Kymmene Corporation with Havas\n",
      "Comparing UPM-Kymmene Corporation with J Sainsbury Plc\n",
      "Comparing UPM-Kymmene Corporation with LAMBERT SMITH HAMPTON\n",
      "Comparing UPM-Kymmene Corporation with Norwegian Property ASA\n",
      "Comparing UPM-Kymmene Corporation with Nouvelle Societe Mineral Products Internation\n",
      "Comparing UPM-Kymmene Corporation with Pandora A/S\n",
      "Comparing UPM-Kymmene Corporation with Phoenix Group Holdings\n",
      "Comparing UPM-Kymmene Corporation with Recordati SpA\n",
      "Comparing UPM-Kymmene Corporation with SGS Société Générale de Surveillance SA\n",
      "Comparing UPM-Kymmene Corporation with SKF\n",
      "Comparing UPM-Kymmene Corporation with SSAB\n",
      "Comparing UPM-Kymmene Corporation with Severn Trent\n",
      "Comparing UPM-Kymmene Corporation with Softcat\n",
      "Comparing UPM-Kymmene Corporation with Sonaca\n",
      "Comparing UPM-Kymmene Corporation with TDC NET A/S\n",
      "Comparing UPM-Kymmene Corporation with Vistry Group plc\n",
      "Comparing UPM-Kymmene Corporation with Vitesco Technologies\n",
      "Comparing UPM-Kymmene Corporation with WHISTL UK LTD\n",
      "Comparing UPM-Kymmene Corporation with thyssenkrupp AG\n",
      "Comparing Borregaard ASA with AMG Advanced Metallurgical Group NV\n",
      "Comparing Borregaard ASA with Amadeus IT Group, S.A.\n",
      "Comparing Borregaard ASA with Coloplast A/S\n",
      "Comparing Borregaard ASA with DUCRON CONSTRUCTION\n",
      "Comparing Borregaard ASA with Ernst & Young Global Ltd\n",
      "Comparing Borregaard ASA with Havas\n",
      "Comparing Borregaard ASA with J Sainsbury Plc\n",
      "Comparing Borregaard ASA with LAMBERT SMITH HAMPTON\n",
      "Comparing Borregaard ASA with Norwegian Property ASA\n",
      "Comparing Borregaard ASA with Nouvelle Societe Mineral Products Internation\n",
      "Comparing Borregaard ASA with Pandora A/S\n",
      "Comparing Borregaard ASA with Phoenix Group Holdings\n",
      "Comparing Borregaard ASA with Recordati SpA\n",
      "Comparing Borregaard ASA with SGS Société Générale de Surveillance SA\n",
      "Comparing Borregaard ASA with SKF\n",
      "Comparing Borregaard ASA with SSAB\n",
      "Comparing Borregaard ASA with Severn Trent\n",
      "Comparing Borregaard ASA with Softcat\n",
      "Comparing Borregaard ASA with Sonaca\n",
      "Comparing Borregaard ASA with TDC NET A/S\n",
      "Comparing Borregaard ASA with Vistry Group plc\n",
      "Comparing Borregaard ASA with Vitesco Technologies\n",
      "Comparing Borregaard ASA with WHISTL UK LTD\n",
      "Comparing Borregaard ASA with thyssenkrupp AG\n",
      "LogProbs have been written to pairwise_testing_new_prompt/testing_with_explanation/A_List_Comparisons_12.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# input files:\n",
    "#   input_file - 2022 merged dataset with all valid European company responses\n",
    "#   companies_to_look_at - company responses you want to score\n",
    "#       we did A list company responses and a stratified set of 147 companies\n",
    "\n",
    "input_file = 'merged_files/2022_merged_dataset.csv'\n",
    "companies_to_look_at = \"a-list_data_files/2022_a_list.csv\"\n",
    "\n",
    "# output_file format: Company A, Company B, Industry, Decision, Token 1, LogProb 1, Token 2, LogProb 2\n",
    "output_file = \"pairwise_testing_new_prompt/testing_with_explanation/A_List_Comparisons_12.csv\"\n",
    "\n",
    "# 24 comparison companies from a stratified set that each company gets compared to\n",
    "comparison_set = [\n",
    "    # insert companies you want to compare against\n",
    "    \"Pandora A/S\", \"thyssenkrupp AG\", \"J Sainsbury Plc\", \"AMG Advanced Metallurgical Group NV\", \n",
    "    \"Sonaca\", \"Havas\", \"Norwegian Property ASA\", \"Phoenix Group Holdings\", \n",
    "    \"SGS Société Générale de Surveillance SA\", \"TDC NET A/S\", \"DUCRON CONSTRUCTION\", \n",
    "    \"Vistry Group plc\", \"LAMBERT SMITH HAMPTON\", \n",
    "    \"Nouvelle Societe Mineral Products Internation\", \"Recordati SpA\", \"SKF\", \n",
    "    \"Amadeus IT Group, S.A.\", \"Vitesco Technologies\", \"SSAB\", \n",
    "    \"Coloplast A/S\", \"Severn Trent\", \"Ernst & Young Global Ltd\", \n",
    "    \"Softcat\", \"WHISTL UK LTD\"\n",
    "]\n",
    "\n",
    "# Load datasets\n",
    "df = pd.read_csv(input_file)\n",
    "a_df = pd.read_csv(companies_to_look_at)\n",
    "\n",
    "# Extract columns\n",
    "company_name_list = df.loc[:, \"Organization\"]\n",
    "industry_list = df.loc[:, 'Primary industry']\n",
    "a_response_list = df.loc[:, a_2022] \n",
    "b_response_list = df.loc[:, b_2022]  \n",
    "\n",
    "# Extract the companies to compare (from the list of 25 companies)\n",
    "companies_to_look_at_list = a_df[\"Company Name\"].tolist()\n",
    "\n",
    "fields = [\"Company A\", \"Company B\", \"Industry\", \"Decision\", \"Token 1\", \"LogProb 1\", \"Token 2\", \"LogProb 2\"]\n",
    "\n",
    "# Write output to CSV\n",
    "with open(output_file, 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fields, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Loop through the 147 companies\n",
    "    for index, companyA in enumerate(companies_to_look_at_list):\n",
    "        \n",
    "        # Check if the current company exists in the dataset\n",
    "        if companyA in company_name_list.values:\n",
    "            row = df[df[\"Organization\"] == companyA].iloc[0]  # Get the row for the current company\n",
    "            industry = row[\"Primary industry\"]\n",
    "            response1a = row[a_2022]\n",
    "            response1b = row[b_2022]\n",
    "            \n",
    "            if pd.notna(industry) and industry != \"\" and isinstance(industry, str):\n",
    "                # Compare against the fixed list of 24 companies\n",
    "                filtered_df = df[df[\"Organization\"].isin(comparison_set)]\n",
    "                a_random_response_list = filtered_df.loc[:, a_2022]\n",
    "                b_random_response_list = filtered_df.loc[:, b_2022]\n",
    "                random_company_name_list = filtered_df.loc[:, \"Organization\"]\n",
    "\n",
    "                # Loop through the 24 fixed companies\n",
    "                for random_list_index in range(len(filtered_df)):\n",
    "                    response2a = a_random_response_list.iloc[random_list_index]\n",
    "                    response2b = b_random_response_list.iloc[random_list_index]\n",
    "                    companyB = random_company_name_list.iloc[random_list_index]\n",
    "\n",
    "                    print(f\"Comparing {companyA} with {companyB}\")\n",
    "\n",
    "                    # Call the function to get log probabilities and tokens\n",
    "                    answer1, list_log_probs = get_log_prob(response1a, response1b, response2a, response2b, companyA, companyB)\n",
    "                    if list_log_probs:\n",
    "                        # Write data to CSV\n",
    "                        line = {\n",
    "                            \"Company A\": companyA, \n",
    "                            \"Company B\": companyB, \n",
    "                            \"Industry\": industry, \n",
    "                            \"Decision\": answer1, \n",
    "                            \"Token 1\": list_log_probs[0][0], \n",
    "                            \"LogProb 1\": list_log_probs[0][1], \n",
    "                            \"Token 2\": list_log_probs[1][0],\n",
    "                            \"LogProb 2\": list_log_probs[1][1]\n",
    "                        }\n",
    "                        writer.writerow(line)\n",
    "                    else:\n",
    "                        print(f\"Insufficient log probabilities for comparison between {companyA} and {companyB}.\")\n",
    "\n",
    "print(f\"LogProbs have been written to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script processes the csv file from above and computes the average log prob for each company.\n",
    "\n",
    "Outputs a file with Company and Average (pairwise score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average log probabilities have been calculated and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "input_file = \"pairwise_testing_new_prompt/with_explanation/147_Non_A_List_Comparisons.csv\"\n",
    "output_file = \"pairwise_testing_new_prompt/with_explanation/Averages_147_Non_A_List.csv\"\n",
    "\n",
    "# Read input CSV\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Initialize tracking variables\n",
    "log_prob_data = {}\n",
    "current_company = None\n",
    "total_score = 0.0\n",
    "total_count = 0\n",
    "\n",
    "# Iterate over each row\n",
    "for index, row in df.iterrows():\n",
    "    company_a = row['Company A']\n",
    "    token_1 = row['Token 1'].strip()\n",
    "    logprob_1 = row['LogProb 1']\n",
    "    logprob_2 = row['LogProb 2']\n",
    "\n",
    "    # Check for a new company\n",
    "    if company_a != current_company:\n",
    "        # Finalize the previous company's calculations\n",
    "        if current_company is not None and total_count > 0:\n",
    "            avg_log_prob = total_score / total_count\n",
    "            log_prob_data[current_company] = avg_log_prob\n",
    "\n",
    "        # Reset counters for the new company\n",
    "        current_company = company_a\n",
    "        total_score = 0.0\n",
    "        total_count = 0\n",
    "\n",
    "    # Add probabilities for both scenarios\n",
    "    if token_1 == 'A':\n",
    "        total_score += logprob_1\n",
    "    else:  # Token 1 is 'B'\n",
    "        total_score += logprob_2\n",
    "    total_count += 1\n",
    "\n",
    "# Finalize the last company's calculations\n",
    "if current_company is not None and total_count > 0:\n",
    "    avg_log_prob = total_score / total_count\n",
    "    log_prob_data[current_company] = avg_log_prob\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "log_prob_df = pd.DataFrame([\n",
    "    {'Company': company, 'Average': avg_log_prob}\n",
    "    for company, avg_log_prob in log_prob_data.items()\n",
    "])\n",
    "\n",
    "# Save to CSV\n",
    "log_prob_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Average log probabilities have been calculated and saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
