{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f4611a3",
   "metadata": {},
   "source": [
    "Title: Rating Evaluation\n",
    "\n",
    "Overview:\n",
    "This script evaluates climate disclosure responses of companies using OpenAI's GPT-4o-mini model. We tell the prompt to judge company's responses based on action taken, specificity, and clarity, while ignoring length, order, and missing data.\n",
    "\n",
    "Main Features:\n",
    "- Uses a rating prompt to guide GPT-4o-mini in evaluating each company's response.\n",
    "- Gets token-level log probability analysis to assess model confidence in answer.\n",
    "- Outputs a file with explanations (if applicable), decision (numerical rating on a scale of 1 to 5), and its respective log probs\n",
    "- We then use those log probs and calculate a final 'score', which gets outputted into a new file\n",
    "\n",
    "How to Run:\n",
    "1. Install dependencies:\n",
    "    openai\n",
    "    pandas\n",
    "    numpy\n",
    "\n",
    "2. Set up your OpenAI API key in `api_keys.py`.\n",
    "    ex. # api_keys.py\n",
    "        OPEN_AI_API_KEYS = [\"your-api-key-here\"]\n",
    "\n",
    "3. Adjust the input and output file paths as needed in the script.\n",
    "\n",
    "\n",
    "Note:\n",
    "Make sure the company responses are aligned with the model's input requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a6b83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from api_keys import OPEN_AI_API_KEYS\n",
    "MODEL = 'gpt-4o-mini'\n",
    "key = OPEN_AI_API_KEYS[0]\n",
    "client = openai.OpenAI(api_key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7846c61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc75e547",
   "metadata": {},
   "source": [
    "Calls OpenAI chat API with full control over parameters like temperature, max tokens, stop sequences, tools, and log probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e41518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(\n",
    "    messages: list[dict[str, str]],\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    max_tokens=500,\n",
    "    temperature=0,\n",
    "    stop=None,\n",
    "    seed=123,\n",
    "    tools=None,\n",
    "    logprobs=None,  # whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message..\n",
    "    top_logprobs=None,\n",
    ") -> str:\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop\": stop,\n",
    "        \"seed\": seed,\n",
    "        \"logprobs\": logprobs,\n",
    "        \"top_logprobs\": top_logprobs,\n",
    "    }\n",
    "    if tools:\n",
    "        params[\"tools\"] = tools\n",
    "\n",
    "    completion = client.chat.completions.create(**params)\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1f298b",
   "metadata": {},
   "source": [
    "Setup of files to read in and result files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed5ba3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6db163",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = data_file.index\n",
    "\n",
    "# Create results csv file\n",
    "result_file = result_file_name\n",
    "fields = [\"Company\", \"Industry\", \"Output\", \"Result 1-1\", \"Percent 1-1\", \"Result 1-2\", \"Percent 1-2\", \"Result 1-3\", \"Percent 1-3\", \"Result 1-4\", \"Percent 1-4\", \"Result 1-5\", \"Percent 1-5\"]\n",
    "\n",
    "# prepare other parts of prompt\n",
    "question_4a = data_file.columns[5]\n",
    "question_4b = data_file.columns[6]\n",
    "\n",
    "response_list_4a = data_file[question_4a]\n",
    "response_list_4b = data_file[question_4b]\n",
    "company_list = data_file[\"Organization\"]\n",
    "industry_list = data_file[\"Primary industry\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61140a2e",
   "metadata": {},
   "source": [
    "All prompt parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81de05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt_start = \"\"\"\n",
    "Please act as an impartial judge and evaluate the quality of the following response provided by a company to the following questions. The company may answer both questions or just one.\n",
    "\"\"\"\n",
    "\n",
    "final_prompt_general = \"\"\"\n",
    "Your evaluation should be primarily based on the tangible action that has already been taken, and can also consider planned actions for the future. Your evaluation should also consider factors such as the specificity, clarity, completeness, and depth of their responses.\n",
    "Do not allow the length of the responses to influence your evaluation. Do not allow a missing response to influence your decision, but simply ignore it and focus on the other response. Do not fall for greenwashing tactics.\n",
    "Please rate this response on a scale of 1-5, based on the considerations above.\n",
    "\"\"\"\n",
    "\n",
    "final_prompt_reference_3 = \"\"\"\n",
    "Following is an example of a response that should receive a score of 3. Use this response as a reference for your decision. \n",
    "\"\"\"\n",
    "example_response_a_3 = \"To grow our business sustainably, we are cutting carbon and maximising energy efficiency. The Science Based Targets initiative (SBTi) has approved our Science Based Targets for Scopes 1, 2 in line with a 1.5-degree pathway and Scope 3 in line with a well below 2-degree pathway.For Scopes 1 and 2, this includes the reduction of greenhouse gas (GHG) emissions from our own operations, aligned with a temperature pathway to limit global warming to 1.5°C. We have committed to achieve Net Zero by 2035. For Scope 3 we are reducing our GHG emissions in the value chain by 30 per cent by 2030.To grow our business sustainably, we are cutting carbon and maximising energy efficiency. The Science Based Targets initiative (SBTi) has approved our Science Based Targets for Scopes 1, 2 in line with a 1.5-degree pathway and Scope 3 in line with a well below 2-degree pathway.For Scopes 1 and 2, these include the reduction of greenhouse gas (GHG) emissions from our own operations, aligned with a temperature pathway to limit global warming to 1.5°C and we have committed to achieve Net Zero by 2035. For Scope 3 we are reducing our GHG emissions in the value chain by 30 per cent by 2030 and this includes reducing emissions from purchased goods, upstream transport and distribution, services sold and our customers'  direct use and consumption of the products we sell.\"\n",
    "example_response_b_3 = \"\"\n",
    "\n",
    "final_prompt_reference_5 = \"\"\"\n",
    "Following is an example of a response that should receive a score of 5. Use this response as a reference for your decision. \n",
    "\"\"\"\n",
    "\n",
    "example_response_a_5 = \"In 2019 we achieved our original 2030 SBT 11 years early, reducing our carbon intensity by 48% since 2014. In line with our aim to lead our sector, in 2019 we became the first UK REIT to increase the ambition level of our science-based carbon reduction target, aligning it to a 1.5-degree scenario (1.5DS). This commitment is the foundation of our transition to net zero.Our updated science-based target, in line with the 1.5DS, is to reduce our absolute carbon emissions (tCO2e) by 70% by 2030 compared to a 2013/14 baseline, for property under our management for at least two years, excluding those properties which are acquired, sold or included in the development pipeline at any time within the last two years. We understand that this two-year period reflects the amount of time needed to undertake sustainability assessments and start implementing changes to the assets; once properties complete the minimum required time under our operational control, they will be included into the commitment portfolio at the start of the following reporting year. This target includes Scope 1 and 2 emissions, and Scope 3 emissions associated with downstream leased assets (gas and electricity procured by us and used by our occupiers) but excludes Scope 1 emissions associated with refrigerant gas. To develop this target, the Absolute Contraction Approach was adopted, which applies the annual emission reduction pathway aligned to a 1.5DS to the baseline emissions of the company, and the pathway is defined by a 4.2% annual linear reduction, which has been derived by the Science Based Targets initiative (SBTi). We worked with the Carbon Trust in order to calculate the emissions pathway for our SBT; the annual reduction aligned to the 1.5DS was applied to our baseline footprint, resulting in the absolute emissions pathway and reduction targets.\"\n",
    "example_response_b_5 = \"\"\n",
    "\n",
    "final_prompt_indicative = \"\"\"\n",
    "Here is the scale you should use to build your answer:\n",
    "1. The response is very poor: it has no targets and/or plans to meet targets, and no progress\n",
    "2. The response is poor: it has targets and/or plans to meet targets, but has made little or no progress\n",
    "3. The response is average: it has targets and/or plans to meet targets, and has made some small or initial progress \n",
    "4. The response is good: it has targets, plans to meet targets, and has made good progress\n",
    "5. The response is excellent: it has clear targets, concrete plans to meet targets, and has made strong progress\n",
    "\"\"\"\n",
    "\n",
    "final_prompt_end_explanation = \"\"\"\n",
    "Before giving your answer, please provide a short explanation of 40 or fewer words discussing the factors that contributed to your decision.\n",
    "After giving your explanation, output your final verdict by strictly following this format of a single number 1, 2, 3, 4, or 5. It should be a single character without quotations or spaces. \n",
    "\"\"\"\n",
    "\n",
    "final_prompt_end = \"\"\"\n",
    "Please do not provide an explanation. \n",
    "Output your final verdict by strictly following this format of a single number 1, 2, 3, 4, or 5. It should be a single character without quotations or spaces. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90723fb",
   "metadata": {},
   "source": [
    "CDP questionairre questions to use in prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518c3064",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_4a_22 = \"Question 1: Provide details of your absolute emissions target(s) and progress made against those targets. - Please explain target coverage and identify any exclusions\"\n",
    "question_4b_22 = \"Question 2: Provide details of your emissions intensity target(s) and progress made against those target(s). - Please explain target coverage and identify any exclusions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ae6220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if it crashes partway through, can rerun and this segment will keep track of companies already seen\n",
    "# if company i is already in csv, move to next index\n",
    "processed_companies = set()\n",
    "try:\n",
    "    with open(result_file, 'r', newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            processed_companies.add(row[\"Company\"])\n",
    "except FileNotFoundError:\n",
    "    # If the file doesn't exist, no indices are processed yet\n",
    "    pass\n",
    "\n",
    "\n",
    "with open(result_file, 'a', newline='') as csvfile:\n",
    "\n",
    "    # creating a csv dict writer object\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fields, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "    if csvfile.tell() == 0:\n",
    "        writer.writeheader()\n",
    "\n",
    "    # get responses one at a time and get decision\n",
    "    for i in indices:\n",
    "        \n",
    "        if company_list[i] in processed_companies:\n",
    "            continue\n",
    "                    \n",
    "        answer_a = response_list_4a[i]\n",
    "        answer_b = response_list_4b[i]\n",
    "\n",
    "        # comment out whichever sections needed depending on prompt configuration\n",
    "        full_prompt = [\n",
    "                    final_prompt_start,\n",
    "\n",
    "                    question_4a_22,\n",
    "                    question_4b_22, \n",
    "\n",
    "                    final_prompt_general,\n",
    "\n",
    "                    # if using explanations\n",
    "                    # \"Please strictly keep your explanation under 40 words.\",\n",
    "\n",
    "                    # final_prompt_reference_3,\n",
    "                    # f\"[The Start of Question 1 Answer] {example_response_a_3} [The End of Question 1 Answer]\",\n",
    "                    # f\"[The Start of Question 2 Answer] {example_response_b_3} [The End of Question 2 Answer]\",\n",
    "\n",
    "                    final_prompt_reference_5,\n",
    "                    f\"[The Start of Question 1 Answer] {example_response_a_5} [The End of Question 1 Answer]\",\n",
    "                    f\"[The Start of Question 2 Answer] {example_response_b_5} [The End of Question 2 Answer]\",\n",
    "\n",
    "                    # final_prompt_indicative,\n",
    "                    \n",
    "                    # use either end or end_explanation below\n",
    "                    # final_prompt_end_explanation,\n",
    "                    final_prompt_end, \n",
    "\n",
    "                    f\"[The Start of Question 1 Answer] {answer_a} [The End of Question 1 Answer]\",\n",
    "                    f\"[The Start of Question 2 Answer] {answer_b} [The End of Question 2 Answer]\"\n",
    "\n",
    "                    ]\n",
    "\n",
    "        full_prompt_string = ' '.join(full_prompt)\n",
    "\n",
    "        decision = []\n",
    "        percent = []\n",
    "        \n",
    "        output = get_completion(\n",
    "            [{\"role\": \"user\", \"content\": full_prompt_string}],\n",
    "            model=\"gpt-4o-mini\",\n",
    "            logprobs=True,\n",
    "            top_logprobs=5,\n",
    "        )\n",
    "\n",
    "        # Extract the actual output text\n",
    "        generated_text = output.choices[0].message.content\n",
    "\n",
    "        print(len(generated_text.split()))\n",
    "        # extract logprobs\n",
    "\n",
    "        # without explanation\n",
    "        # top_two_logprobs = output.choices[0].logprobs.content[0].top_logprobs\n",
    "        \n",
    "        # with explanation\n",
    "        top_two_logprobs = output.choices[0].logprobs.content[-1].top_logprobs\n",
    "\n",
    "\n",
    "        for j, logprob in enumerate(top_two_logprobs, start=1):\n",
    "            decision.append(logprob.token)\n",
    "            percent.append(np.round(np.exp(logprob.logprob)*100,2))\n",
    "\n",
    "        # write new line to csv file\n",
    "        line = {\"Company\":company_list[i], \"Industry\": industry_list[i], \"Output\":generated_text, \"Result 1-1\": decision[0], \"Percent 1-1\":percent[0], \"Result 1-2\": decision[1], \"Percent 1-2\": percent[1], \"Result 1-3\": decision[2], \"Percent 1-3\":percent[2], \"Result 1-4\": decision[3], \"Percent 1-4\": percent[3], \"Result 1-5\": decision[4], \"Percent 1-5\":percent[4]}\n",
    "        writer.writerow(line)\n",
    "\n",
    "    csvfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df191f08",
   "metadata": {},
   "source": [
    "Convert log probs to weighted average between 1 and 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a46dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to cast to int\n",
    "def safe_cast_to_int(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except ValueError:\n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15fe357",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input = \"sample_set_log_probs.csv\"\n",
    "output = \"sample_set_averages.csv\"\n",
    "\n",
    "# get data\n",
    "df_non_a = pd.read_csv(input)\n",
    "companies_average = {}\n",
    "\n",
    "for company_number in range(len(df_non_a)):\n",
    "    average = 0\n",
    "    for rating_number in range(1,6):\n",
    "        rating_column = \"Result 1-\" + str(rating_number)\n",
    "        percent_column = \"Percent 1-\" + str(rating_number)\n",
    "\n",
    "        rating = df_non_a[rating_column].iloc[company_number]\n",
    "        if isinstance(rating, np.int64):\n",
    "            rating = int(rating)\n",
    "        if isinstance(rating, str):\n",
    "            rating = safe_cast_to_int(rating)\n",
    "        \n",
    "        if isinstance(rating, int):\n",
    "            average += rating * (df_non_a[percent_column].iloc[company_number]/100)\n",
    "        companies_average[df_non_a['Company'].iloc[company_number]] = average\n",
    "\n",
    "df_average_companies = pd.DataFrame(list(companies_average.items()), columns=['Company', 'Average'])\n",
    "\n",
    "df_average_companies.to_csv(output, index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
